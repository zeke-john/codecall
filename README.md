# Codecall

> An open source Typescript implementation of Programmatic Tool Calling for AI Agents.

Codecall changes how agents interact with tools by letting them **write and execute code** instead of making individual tool calls that bloat context, increase the price, and slow everything down

Works with **MCP servers** and **standard tool definitions**.

> [!NOTE]
> **Before reading** :)
>
> Please keep in mind all of this is the **future plan** for Codecall and how it will work. Codecall is still a WIP and not production ready.
>
> The README describes the vision and architecture for how the system will function once completed and worked on. Features, API design, and implementation details are subject to change.
>
> If you're interested in contributing or following the project, check back soon or open an issue to discuss ideas!

## The Problem

Traditional tool calling has fundamental architectural issues that get worse at scale:

### 1. Context Bloat

Every tool definition lives in your system prompt. Connect a few MCP servers and you're burning tens of thousands of tokens before the conversation even starts.

```
GitHub MCP:        16 tools  â†’  ~32,000 tokens
Internal Tools:    12 tools  â†’  ~24,000 tokens
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:             28 tools  â†’  ~56,000 tokens (before any work happens)
```

### 2. Inference Overhead

Each tool call requires a full model inference pass. The entire conversation history gets sent back and forth every single time.

```
User: "Find all admin users and update their permissions"

Traditional approach:
  Turn 1: [8,000 tokens] â†’ get_all_users()
  Turn 2: [18,000 tokens] â†’ filter mentally, pick first admin
  Turn 3: [19,500 tokens] â†’ update_user(id1, ...)
  Turn 4: [21,000 tokens] â†’ update_user(id2, ...)
  Turn 5: [22,500 tokens] â†’ update_user(id3, ...)
  ...
  Total: 150,000+ tokens, 12 inference passes
```

The problem also compounds because each tool call adds its output to the context, making every subsequent generation more expensive.

### 3. Models Are Bad at Data Lookup

Benchmarks show models have a **10-50% failure rate** when searching through large datasets in context. They hallucinate field names, miss entries, and get confused by similar data.

But doing this programmatically fixes this because it can just write code, as its deterministic (so 0% failure rate)

```typescript
users.filter((u) => u.role === "admin");
```

### 4. Models were never trained for tool calling

The special tokens used for tool calls (`<tool_call>`, `</tool_call>`) are synthetic training data. Models dont have much exposure to the tool calling syntax, and have only seen contrived examples from training sets... but they DO have:

- Millions of lines of real world TypeScript
- Lots of experience writing code to call APIs

> â€œMaking an LLM perform tasks with tool calling is like putting Shakespeare through a month-long class in Mandarin and then asking him to write a play in it. Itâ€™s just not going to be his best work.â€  
> â€” [Cloudflare Engineering](https://blog.cloudflare.com/code-mode/)

#### An example of a model that WAS trained to call tools

Even though Grok 4 was heavily trained on tool calling. Result? It hallucinates tool call XML syntax in the middle of responses, writing the format but not triggering actual execution. The model â€œknowsâ€ the syntax exists but doesnâ€™t use it correctly.

## The Solution

Let models do what they're good at: **writing code**.

LLMs have enormous amounts of real-world TypeScript in their training data. They're significantly better at writing code to call APIs than they are at the arbitrary JSON matching that tool calling requires.

```typescript
// Instead of 12+ inference passes and 150+ tokens:
const allUsers = await tools.users.listAllUsers();
const adminUsers = allUsers.filter((u) => u.role === "admin");
const resources = await tools.resources.getSensitiveResources();

progress({
  step: "Data loaded",
  admins: adminUsers.length,
  resources: resources.length,
});

const revokedAccesses = [];
const failedAccesses = [];

for (const admin of adminUsers) {
  for (const resource of resources) {
    try {
      const result = await tools.permissions.revokeAccess({
        userId: admin.id,
        resourceId: resource.id,
      });
      if (result.success) {
        revokedAccesses.push({ admin: admin.name, resource: resource.name });
      }
    } catch (err) {
      failedAccesses.push({
        admin: admin.name,
        resource: resource.name,
        error: err.message,
      });
    }
  }
}

return {
  totalAdmins: adminUsers.length,
  resourcesAffected: resources.length,
  accessesRevoked: revokedAccesses.length,
  accessesFailed: failedAccesses.length,
};
```

One inference pass. ~2,000 tokens. 98.7% reduction.

## Main Challenges (Recently Updated)

### Output Schemas from Tools

MCP tool definitions include `inputSchema` (what you pass to a tool) but `outputSchema` is **optional** and most servers almost never provide it.

**Why this matters:** Codecall generates TypeScript code that chains tool calls together. Without knowing what a tool returns, the model has to guess the structure, leading to runtime errors.

**Example of the problem:**

```typescript
const tasks = await tools.todoist.getTasks({ filter: "today" });

for (const task of tasks) {
  console.log(task.title);  // BUG: actual property is "name", not "title"
}

if (task.dueDate === "2024-01-15") { ... }
// BUG: actual structure is task.due, not task.dueDate
```

The code looks correct but fails at runtime because the model hallucinated the return type based on common naming patterns...

**OUR WORKAROUND:**
We haven't fully solved this (that would require MCP servers to provide `outputSchema`), but we've implemented a hack that works in practice:

1. **Tool Classification** - We use an LLM to classify each tool as `read`, `write`, `destructive`, or `write_read` based on its semantics
2. **Output Schema Discovery** - For tools classified as `read` or `write_read`, we generate safe sample inputs and actually call the tool
3. **Schema Inference** - We capture the real response and infer a JSON schema from it
4. **Typed SDK Generation** - The inferred schema is passed to the SDK generator, producing proper TypeScript output types

This means tools like `search_engine` now generate SDKs with accurate output types based on real API responses, not guesses.

**Limitations:**

- Requires actually calling the tools during SDK generation
- Single sample responses may miss optional fields or variant shapes
- Write+Read tools create real data (we use identifiable test names like `codecall_test_*`)

---

### Tool Outputs Are Often Plain Strings (MAIN BLOCKER)

A second more fundamental challenge is that a lot of MCP servers return plain strings or markdown, not structured data...

In these cases:

- The output has no stable shape
- There are no fields to index into
- There is nothing meaningful to type beyond string

From Codecallâ€™s perspective, this means:

- No reliable code generation beyond simple passthrough
- No safe composition of tool outputs
- No advantage over a traditional agent that directly interprets text

This is not a limitation of Codecall, but a reflection of how the tools were designed.

Because Codecall focuses on deterministic, type-safe code generation, its benefits disappear when tool outputs are unstructured. In those cases, interpretation must happen in the language model itself, which moves the system back into standard agent behavior.

There is no reliable workaround when using external MCP servers: if you do not control the tool, you cannot enforce structured outputs.

---

## How Codecall Works (WIP)

Instead of exposing every tool directly to the LLM for it to call, Codecall:

- Converts your MCP definitions into TypeScript SDK files (types + function signatures)
- Shows the model a directory tree of available files
- Allows the model to selectively read SDK files to understand types and APIs
- Lets the model write code to accomplish the task
- Executes that code in a deno sandbox with access to your actual tools as functions
- Returns the execution result back to the same model in a 2nd request
- Lets the model produce the final user-facing response

Codecall uses two model requests per user turn: one to write code, and one to explain the execution result.

### SDK File Tree (What the Model Sees)

At the start of every request, the model is shown only the file tree, not the contents of each file.

```
tools/
â”œâ”€ users/
â”‚ â”œâ”€ listAllUsers.ts
â”‚ â”œâ”€ getUser.ts
â”‚ â”œâ”€ updateUser.ts
â”‚ â””â”€ ...
â”œâ”€ permissions/
â”‚ â”œâ”€ revokeAccess.ts
â”‚ â”œâ”€ grantAccess.ts
â”‚ â”œâ”€ listPermissions.ts
â”‚ â””â”€ ...
â””â”€ resources/
â”œâ”€ getSensitiveResources.ts
â”œâ”€ listResources.ts
â””â”€ ...
```

This gives the model a high-level map of what capabilities exist without bloating the prompt with thousands of lines of schemas.

### Reading SDK Files (Discovering Tools)

When the model needs to understand how a specific tool works, it can explicitly request the contents of a file using a built in `readFile` tool:

```typescript
readFile({
  module: "users",
  name: "listAllUsers",
});
```

This returns the entire contents of that SDK file, and each file contains type definitions for that tool, for example:

```typescript
// /tools/users/listAllUsers.ts
// SDK stub for tool: "users.listAllUsers"

export interface ListAllUsersInput {
  limit?: number;
  offset?: number;
}

export interface User {
  id: string;
  name: string;
  email: string;
  role: "admin" | "user" | "guest";
  department: string;
  createdAt: string;
}

export async function listAllUsers(input: ListAllUsersInput): Promise<User[]> {
  return call("users.listAllUsers", input);
}
```

The model can read only the files it needs which it can infer from the filenames, just like a developer opening files in an editor, instead of having every tool definition injected into context upfront.

This keeps prompts small while still giving the model precise, typed knowledge of each tool before it writes code.

### Writing Code

After reading the relevant SDK files, the model writes a single TypeScript program like:

```typescript
const allUsers = await tools.users.listAllUsers({ limit: 1000 });
progress({ step: "Loaded all users", count: allUsers.length });

const adminUsers = allUsers.filter((u) => u.role === "admin");
progress({ step: "Identified admin users", count: adminUsers.length });

const sensitiveResources = await tools.resources.getSensitiveResources();
progress({
  step: "Loaded sensitive resources",
  count: sensitiveResources.length,
});

const revokedAccesses = [];
const failedAccesses = [];
let processed = 0;

for (const admin of adminUsers) {
  for (const resource of sensitiveResources) {
    processed++;
    try {
      const result = await tools.permissions.revokeAccess({
        userId: admin.id,
        resourceId: resource.id,
        reason: "security-audit",
      });

      if (result.success) {
        revokedAccesses.push({
          adminName: admin.name,
          adminEmail: admin.email,
          resourceName: resource.name,
          revokedAt: result.timestamp,
        });
      } else {
        failedAccesses.push({
          adminName: admin.name,
          resourceName: resource.name,
          reason: result.reason || "unknown",
        });
      }

      if (processed % 10 === 0) {
        progress({
          step: "Revoking access",
          processed,
          total: adminUsers.length * sensitiveResources.length,
        });
      }
    } catch (err) {
      failedAccesses.push({
        adminName: admin.name,
        resourceName: resource.name,
        error: err.message,
      });
    }
  }
}

return {
  summary: {
    totalAdminsAffected: adminUsers.length,
    totalResourcesAffected: sensitiveResources.length,
    totalAccessProcessed: processed,
    accessesRevoked: revokedAccesses.length,
    accessesFailed: failedAccesses.length,
    successRate: Math.round((revokedAccesses.length / processed) * 100),
  },
  revoked: revokedAccesses,
  failed: failedAccesses.slice(0, 20),
};
```

SDK files exist only for the model to get the types and discoverability.

Runtime execution never imports SDK files, it uses a tools bridge injected by Codecall.

### How Tool Calls Work at Runtime

When the generated code runs, Codecall injects a real `tools` object into the sandbox.

- `tools` is not a set of local functions, but it's a small runtime bridge provided by Codecall
- Each call to `tools.*` is forwarded to the real tool implementation

So when the sandbox executes:

```typescript
const result = await tools.permissions.revokeAccess({
  userId: admin.id,
  resourceId: resource.id,
  reason: "security-audit",
});
```

What actually happens is:

- The sandbox captures the tool name (`"communications.sendSecureMessage"`) and arguments
- Codecall forwards that request to the connected MCP server using `tools/call`
- The MCP server executes the real tool
- The result is returned back to the sandbox
- The script continues running

From the codeâ€™s perspective this behaves exactly like calling a normal async function.

### Code Execution & Sandboxing

When the model finishes writing the TypeScript code, Codecall executes that code inside a fresh, short-lived sandbox. Each sandbox is spun up using Deno and runs the code in isolation. Denoâ€™s security model blocks access to sensitive capabilities unless explicitly allowed.

By default, the sandboxed code has no access to the filesystem, network, environment variables, or system processes. The only way it can interact with the outside world is by calling the tool functions exposed through tools (which are forwarded by Codecall to the MCP server).

Every execution is independent. Retries in the recovery loop run in a new sandbox if it errors, which keeps execution fast, predictable, and easy to reason about while preventing state from leaking between runs.

#### Sandbox Lifecycle Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              SANDBOX LIFECYCLE                                          â”‚
â”‚                                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚  SPAWN  â”‚â”€â”€â”€â”€â–¶â”‚  INJECT â”‚â”€â”€â”€â”€â–¶â”‚ EXECUTE â”‚â”€â”€â”€â”€â–¶â”‚ CAPTURE â”‚â”€â”€â”€â”€â–¶â”‚ DESTROY â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚        â”‚               â”‚               â”‚               â”‚               â”‚                â”‚
â”‚        â–¼               â–¼               â–¼               â–¼               â–¼                â”‚
â”‚   Fresh Deno      tools proxy     Run generated    Collect return   Terminate           â”‚
â”‚   process with    + progress()    TypeScript       value or error   process,            â”‚
â”‚   deny-all        injected        code             + exec trace     cleanup             â”‚
â”‚   permissions                                                                           â”‚
â”‚                                                                                         â”‚
â”‚                                                                                         â”‚
â”‚   ON ERROR:                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚                                                                         â”‚           â”‚
â”‚   â”‚   Error + Trace + Original Code + User Request                          â”‚           â”‚
â”‚   â”‚                        â”‚                                                â”‚           â”‚
â”‚   â”‚                        â–¼                                                â”‚           â”‚
â”‚   â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚           â”‚
â”‚   â”‚              â”‚   Model (LLM)   â”‚  Rewrites code to fix issue            â”‚           â”‚
â”‚   â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚           â”‚
â”‚   â”‚                       â”‚                                                 â”‚           â”‚
â”‚   â”‚                       â–¼                                                 â”‚           â”‚
â”‚   â”‚              NEW SANDBOX SPAWNED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Retry (max 3)     â”‚           â”‚
â”‚   â”‚                                                                         â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  DATA FLOW                                              â”‚
â”‚                                                                                         â”‚
â”‚                                                                                         â”‚
â”‚    SANDBOX                        TOOL BRIDGE                         MCP SERVER        â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚  tools.users.listAllUsers()   â”‚                                    â”‚            â”‚
â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚   tools/call: listAllUsers         â”‚            â”‚
â”‚       â”‚                               â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚          [{ id, name, role }, ...] â”‚            â”‚
â”‚       â”‚                               â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚   Promise<User[]> resolved    â”‚                                    â”‚            â”‚
â”‚       â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚  (code continues execution)   â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚  progress({ step: "Done" })   â”‚                                    â”‚            â”‚
â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚                          Streams to UI                             â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚  return { success: true }     â”‚                                    â”‚            â”‚
â”‚       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                                    â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚       â”‚                     Result sent to Model                           â”‚            â”‚
â”‚       â”‚                     for response generation                        â”‚            â”‚
â”‚       â”‚                               â”‚                                    â”‚            â”‚
â”‚                                       â–¼                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Returning Results to the Model

After the sandbox finishes executing the generated code, Codecall returns the final result object, with the same conversation history as the original user request back to the same model so it can produce a user-facing response.

This happens in a second model request, so the flow looks like this:

1. Code Generation (Request 1)

   - Model sees the tool tree and reads SDK files as needed
   - Model writes a single TypeScript script to perform what action is needed

2. Sandbox Execution

   - Codecall runs the program in an isolated sandbox
   - Tools are invoked via `tools.*`
   - A final result object is produced

3. Response Generation (Request 2)
   - The result object is sent back to the same model in another request (#2)
   - The model explains the outcome to the user in natural language, which is the user-facing response

For example, after execution completes:

```typescript
{
  summary: {
    totalAdminsAffected: 12,
    totalResourcesAffected: 5,
    totalAccessProcessed: 60,
    accessesRevoked: 58,
    accessesFailed: 2,
    successRate: 97,
  },
  revoked: [
    { adminName: "Alice Chen", adminEmail: "alice@company.com", resourceName: "Production Database", revokedAt: "2025-12-27T18:05:32Z" },
    { adminName: "Bob Smith", adminEmail: "bob@company.com", resourceName: "Production Database", revokedAt: "2025-12-27T18:05:33Z" },
    // ... 56 more entries (you can modify how much data you want the llm to return with in your prompt)
  ],
  failed: [
    { adminName: "Charlie Davis", resourceName: "API Gateway", reason: "dependency-lock" },
    { adminName: "Diana Wong", resourceName: "Billing System", error: "timeout after 30s" },
  ],
}
```

The model receives that result and responds:

> â€œI successfully revoked admin access across the organization. Out of 60 total access permissions (12 admins across 5 sensitive resources), I revoked 58 successfully with a 97% success rate. 2 revocations failed due to system locks and timeouts that will need manual intervention. The revoked admins include Alice Chen, Bob Smith, and 10 others. Most failures occurred on the API Gateway and Billing System where there are active dependencies preventing immediate access removal.â€

## Handling Errors (v2)

Real world API data is messy so pre-written code can (and will) fail. Codecall handles this with an automatic recovery loop:

### The Recovery Loop

1. **Optimistic Execution**: Codecall tries to run the agent's generated code
2. **Failure Capture**: If the code throws an error, we capture the full execution trace (inputs, outputs, and the specific error)
3. **Feedback Loop**: We feed this trace back to the model
4. **Correction**: The model sees exactly where and why it failed, adjusts its approach, and retries keeping the main task in mind

### Example

**1. Agent writes optimistic code:**

```typescript
const allUsers = await tools.users.listAllUsers();
progress({ step: "Loaded users", count: allUsers.length });

const adminUsers = allUsers.filter((u) => u.role === "admin");
const resources = await tools.resources.getSensitiveResources();
progress({ step: "Loaded resources", count: resources.length });

const revoked = [];
for (const admin of adminUsers) {
  for (const resource of resources) {
    await tools.permissions.revokeAccess(admin.id, resource.id);
    revoked.push({ admin: admin.name, resource: resource.name });
  }
}

return { totalRevoked: revoked.length };
```

**2. Execution fails. Codecall catches and returns:**

```typescript
{
  "status": "failed",
  "error": "ToolError: revokeAccess expected object { userId: string, resourceId: string }, got (string, string)",
  "executionTrace": [
    {
      "step": 1,
      "function": "listAllUsers",
      "input": {},
      "output": { "count": 47, "users": [{ "id": "admin-1", "name": "Alice", "role": "admin" }, ...] }
    },
    {
      "step": 2,
      "progress": "Loaded users",
      "data": { "count": 47 }
    },
    {
      "step": 3,
      "function": "getSensitiveResources",
      "input": {},
      "output": { "count": 5, "resources": [{ "id": "resource-db-prod", "name": "Production Database" }, ...] }
    },
    {
      "step": 4,
      "progress": "Loaded resources",
      "data": { "count": 5 }
    },
    {
      "step": 5,
      "function": "revokeAccess",
      "input": ["admin-1", "resource-db-prod"],
      "error": "Invalid Argument Schema. Expected { userId, resourceId }, got positional arguments"
    }
  ],
  "failurePoint": "step 5",
  "context": "The function revokeAccess was called with positional arguments instead of an object. The sandbox successfully loaded 47 users and 5 resources before failing on the first access revocation."
}
```

**3. Agent self-corrects and retries:**

```typescript
const allUsers = await tools.users.listAllUsers();
progress({ step: "Loaded users", count: allUsers.length });

const adminUsers = allUsers.filter((u) => u.role === "admin");
const resources = await tools.resources.getSensitiveResources();
progress({ step: "Loaded resources", count: resources.length });

const revokedAccesses = [];
const failedAccesses = [];
let processed = 0;

for (const admin of adminUsers) {
  for (const resource of resources) {
    processed++;
    try {
      const result = await tools.permissions.revokeAccess({
        userId: admin.id,
        resourceId: resource.id,
      });

      if (result.success) {
        revokedAccesses.push({
          adminName: admin.name,
          resourceName: resource.name,
        });
      } else {
        failedAccesses.push({
          adminName: admin.name,
          resourceName: resource.name,
          reason: result.reason,
        });
      }

      if (processed % 15 === 0) {
        progress({
          step: "Revoking access",
          processed,
          total: adminUsers.length * resources.length,
        });
      }
    } catch (err) {
      failedAccesses.push({
        adminName: admin.name,
        resourceName: resource.name,
        error: err.message,
      });
    }
  }
}

return {
  summary: {
    totalAdmins: adminUsers.length,
    totalResources: resources.length,
    processed,
    revoked: revokedAccesses.length,
    failed: failedAccesses.length,
  },
  revokedSample: revokedAccesses.slice(0, 10),
  failedDetails: failedAccesses,
};
```

On failure, Codecall returns the error, full execution trace, the TypeScript code which failed, and the original user request with the main task to the model. The sandbox is destroyed after each run, but the trace is preserved.

The model then rewrites the code (in another request) and Codecall reruns it in a fresh sandbox, retrying until it succeeds or hits a configurable limit (default 3). If the limit is reached, Codecall stops and returns a clear failure.

Finally, Codecall returns the user response: on success returns the result object from the code, and on failure it explains where and why it failed so we can let the user know.

## Progress Updates (v2)

Codecall runs code in one shot, but you would still most likely want some user facing intermediate updates between the start and end of a request. The sandbox exposes a `progress()` helper that can log the steps it's taking

So for example, in your system prompt you can tell the model to use `progress()`:

```text
When writing code, use progress(...) to show meaningful updates. can see what is happening. For example:

  progress("Loading data...");
  progress({ step: "Processing", current: i, total });
  progress({ step: "Sending emails", done: count });
```

Agent Code Example

```typescript
const allUsers = await tools.users.listAllUsers({ limit: 5000 });
progress({
  step: "Loaded all users",
  totalCount: allUsers.length,
  adminCount: allUsers.filter((u) => u.role === "admin").length,
});

const adminUsers = allUsers.filter((u) => u.role === "admin");
const sensitiveResources = await tools.resources.getSensitiveResources();
progress({
  step: "Loaded sensitive resources",
  resourceCount: sensitiveResources.length,
  resourceNames: sensitiveResources.map((r) => r.name),
});

const revokedAccesses = [];
const failedAccesses = [];

for (let i = 0; i < adminUsers.length; i++) {
  const admin = adminUsers[i];

  for (let j = 0; j < sensitiveResources.length; j++) {
    const resource = sensitiveResources[j];

    try {
      const result = await tools.permissions.revokeAccess({
        userId: admin.id,
        resourceId: resource.id,
        reason: "security-audit",
      });

      if (result.success) {
        revokedAccesses.push({
          admin: admin.name,
          email: admin.email,
          resource: resource.name,
          timestamp: result.timestamp,
        });
      } else {
        failedAccesses.push({
          admin: admin.name,
          resource: resource.name,
          reason: result.reason || "unknown",
        });
      }

      if (((i + 1) * (j + 1)) % 10 === 0) {
        progress({
          step: "Revoking access",
          admin: admin.name,
          resource: resource.name,
          processed: revokedAccesses.length + failedAccesses.length,
          revoked: revokedAccesses.length,
          failed: failedAccesses.length,
        });
      }
    } catch (err) {
      failedAccesses.push({
        admin: admin.name,
        resource: resource.name,
        error: err.message,
      });
    }
  }
}

progress({
  step: "Access revocation complete",
  revoked: revokedAccesses.length,
  failed: failedAccesses.length,
});

return {
  execution: {
    totalAdminsProcessed: adminUsers.length,
    totalResourcesAffected: sensitiveResources.length,
    totalAttempted: revokedAccesses.length + failedAccesses.length,
    accessesRevoked: revokedAccesses.length,
    accessesFailed: failedAccesses.length,
    successPercentage: Math.round(
      (revokedAccesses.length /
        (revokedAccesses.length + failedAccesses.length)) *
        100
    ),
  },
  revokedDetails: revokedAccesses.map((r) => ({
    ...r,
    status: "success",
  })),
  failureDetails: failedAccesses.slice(0, 25),
};
```

This keeps the UX of a "step by step" agent with user facing intermediate updates, while still getting the cost and speed benefits of single-pass execution.

## Why TypeScript?

[Benchmarks](https://github.com/Tencent-Hunyuan/AutoCodeBenchmark?tab=readme-ov-file#experimental-results) show Claude Opus 4.1 performs:

- **42.3%** on Python
- **47.7%** on TypeScript

That's a 12% improvement just from language choice, and various other models show the same pattern.

TypeScript also gives you:

- Full type inference for SDK generation
- Compile time validation of tool schemas
- The model sees types and can use them correctly

## Roadmap

WIP, Please check back soon or feel free to add here :)

Still working on how the high level architecture and how everything should work/flow together

## Contributing

We welcome contributions! Please Feel free to:

- Open issues for bugs or feature requests
- Submit PRs for improvements
- Share your use cases and feedback

## Acknowledgements

This project builds on ideas from the community and is directly inspired by:

#### Videos

- Yannic Kilcher â€“ [What Cloudflare's code mode misses about MCP and tool calling](https://www.youtube.com/watch?v=0bpYCxv2qhw)
- Theo â€“ [Anthropic admits that MCP sucks](https://www.youtube.com/watch?v=1piFEKA9XL0&t=201s) & [Anthropic is trying SO hard to fix MCP...](https://www.youtube.com/watch?v=hPPTrsUzLA8&t=2s)
- Boundary - [Using MCP server with 10000+ tools: ğŸ¦„ Ep #7](https://www.youtube.com/watch?v=P5wRLKF4bt8)

#### Articles

- Cloudflare â€“ [Code mode: the better way to use MCP](https://blog.cloudflare.com/code-mode/)
- Anthropic â€“ [Code execution with MCP: building more efficient AI agents](https://www.anthropic.com/engineering/code-execution-with-mcp) & [Introducing advanced tool use](https://www.anthropic.com/engineering/advanced-tool-use)
- Medium - [Your Agent Is Wasting Money On Tools. Code Execution With MCP Fixes It.](https://medium.com/genaius/your-agent-is-wasting-money-on-tools-code-execution-with-mcp-fixes-it-5c8d7b177bad)

## License

MIT
